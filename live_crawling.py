from flask import Flask, jsonify, request
import requests
from bs4 import BeautifulSoup
from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from datetime import datetime
import threading

app = Flask(__name__)

# âœ… DB ì—°ê²°
DATABASE_URL = "mysql+pymysql://songsungmin:password0419@news-db.cjg2aaai646f.ap-northeast-2.rds.amazonaws.com:3306/newsdb"
engine = create_engine(DATABASE_URL, echo=True)
Base = declarative_base()
Session = sessionmaker(bind=engine)

# âœ… ëª¨ë¸ ì •ì˜
class News(Base):
    __tablename__ = 'news'
    news_id = Column(Integer, primary_key=True, autoincrement=True)
    title = Column(String(500), nullable=False)
    content = Column(Text)
    publisher = Column(String(255), nullable=False)
    published_at = Column(DateTime, nullable=False)
    url = Column(String(500), nullable=False)
    category = Column(String(255), nullable=False)
    image_url = Column(String(500), nullable=False)

Base.metadata.create_all(engine)

# âœ… ì¹´í…Œê³ ë¦¬ URL
category_sections = {
    "ì •ì¹˜": "https://news.naver.com/section/100",
    "ê²½ì œ": "https://news.naver.com/section/101",
    "ì‚¬íšŒ": "https://news.naver.com/section/102",
    "ìƒí™œ/ë¬¸í™”": "https://news.naver.com/section/103",
    "ì„¸ê³„": "https://news.naver.com/section/104",
    "IT/ê³¼í•™": "https://news.naver.com/section/105",
    "all": "https://news.naver.com/"
}

headers = {'User-Agent': 'Mozilla/5.0'}

def sanitize(text):
    return text.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ').strip() if text else text

def parse_datetime(text):
    try:
        return datetime.strptime(text, "%Y.%m.%d. %p %I:%M")
    except:
        return datetime.now()

def is_valid_article_link(href):
    return '/article/' in href and '/comment/' not in href and '/hotissue/' not in href

def clean_article_url(href):
    href = href.replace('/comment', '').split('?')[0]
    return href if href.startswith('http') else 'https://news.naver.com' + href

def trigger_body_crawler(id_url_list):
    try:
        print("ğŸš€ ë³¸ë¬¸ í¬ë¡¤ëŸ¬ í˜¸ì¶œ ì¤‘...")
        res = requests.post(
            "http://localhost:5003/trigger",
            json={"articles": id_url_list}
        )
        print("ğŸ§  ë³¸ë¬¸ í¬ë¡¤ë§ ê²°ê³¼:", res.json())
    except Exception as e:
        print("âŒ ë³¸ë¬¸ í¬ë¡¤ë§ íŠ¸ë¦¬ê±° ì‹¤íŒ¨:", e)

# âœ… ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
def crawl_news(category, max_articles=10):
    section_url = category_sections[category]
    res = requests.get(section_url, headers=headers)
    soup = BeautifulSoup(res.text, 'html.parser')

    # âœ… allì¸ ê²½ìš° ë©”ì¸ì—ì„œ ì£¼ìš” ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
    if category == 'all':
        article_links = {
            clean_article_url(a['href']) for a in soup.find_all('a', href=True)
            if is_valid_article_link(a['href'])
        }
    else:
        article_links = {
            clean_article_url(a['href']) for a in soup.find_all('a', href=True)
            if is_valid_article_link(a['href'])
        }

    session = Session()
    id_url_list = []
    article_list = []

    for link in article_links:
        if len(article_list) >= max_articles:
            break

        try:
            article_res = requests.get(link, headers=headers)
            article_soup = BeautifulSoup(article_res.text, 'html.parser')

            title_tag = article_soup.select_one('h2.media_end_head_headline, div#title_area span')
            press_tag = article_soup.select_one('a.media_end_head_top_logo img')
            date_tag = article_soup.select_one('span.media_end_head_info_datestamp_time, span#date_text')

            og_image = article_soup.select_one('meta[property="og:image"]')
            image_url = og_image['content'] if og_image and og_image.has_attr('content') else ''
            if not image_url:
                img_tag = article_soup.select_one('figure img, #img1, .newsct_article img')
                if img_tag:
                    image_url = img_tag.get('src') or img_tag.get('data-src') or ''

            news = News(
                title=sanitize(title_tag.get_text(strip=True)) if title_tag else "ì œëª© ì—†ìŒ",
                publisher=sanitize(press_tag['alt']) if press_tag and 'alt' in press_tag.attrs else "N/A",
                published_at=parse_datetime(date_tag.get_text(strip=True)) if date_tag else datetime.now(),
                url=link,
                category=category,
                content=None,
                image_url=image_url
            )
            session.add(news)
            session.flush()

            article_list.append({
                "news_id": news.news_id,  
                "title": news.title,
                "url": news.url,
                "publisher": news.publisher,
                "published_at": str(news.published_at),
                "category": news.category,
                "content": news.content,
                "image_url": news.image_url
            })


            id_url_list.append({
                "news_id": news.news_id,
                "url": news.url
            })

        except Exception as e:
            print(f"âŒ [{link}] ì˜¤ë¥˜: {e}")
            continue

    session.commit()
    session.close()

    threading.Thread(target=trigger_body_crawler, args=(id_url_list,)).start()

    return article_list

# âœ… API ë¼ìš°íŠ¸
@app.route('/news', methods=['GET'])
def get_news():
    category = request.args.get('category', default='all')
    limit = int(request.args.get('limit', 10))  # default = 10 â†’ ì¶”ì²œ ì‹œì—ëŠ” 2

    if category not in category_sections:
        return jsonify({"error": "ì˜ëª»ëœ ì¹´í…Œê³ ë¦¬ì…ë‹ˆë‹¤."}), 400

    data = crawl_news(category, max_articles=limit)
    return jsonify({
        "category": category,
        "total": len(data),
        "articles": data
    })


# âœ… ì‹¤í–‰
if __name__ == '__main__':
    print("ğŸ“° live_crawling.py ì‹¤í–‰ ì¤‘...")
    app.run(host='0.0.0.0', port=5001, debug=True)
